% --- Template for thesis / report with tktltiki2 class ---
%
% last updated 2013/02/15 for tkltiki2 v1.02

\documentclass[12pt,finnish]{tktltiki2}

% tktltiki2 automatically loads babel, so you can simply
% give the language parameter (e.g. finnish, swedish, english, british) as
% a parameter for the class: \documentclass[finnish]{tktltiki2}.
% The information on title and abstract is generated automatically depending on
% the language, see below if you need to change any of these manually.
%
% Class options:
% - grading                 -- Print labels for grading information on the front page.
% - disablelastpagecounter  -- Disables the automatic generation of page number information
%                              in the abstract. See also \numberofpagesinformation{} command below.
%
% The class also respects the following options of article class:
%   10pt, 11pt, 12pt, final, draft, oneside, twoside,
%   openright, openany, onecolumn, twocolumn, leqno, fleqn
%
% The default font size is 11pt. The paper size used is A4, other sizes are not supported.
%
% rubber: module pdftex

% --- General packages ---

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsfonts,amsmath,amssymb,amsthm,booktabs,color,enumitem,graphicx}
\usepackage[pdftex,hidelinks]{hyperref}

\usepackage{url}
\usepackage{setspace}
\onehalfspacing

\setlength{\parskip}{1ex} % kappaleiden välinen tyhjä tila
\setlength{\parindent}{0pt} % kappaleen alun sisennys

% Automatically set the PDF metadata fields
\makeatletter
\AtBeginDocument{\hypersetup{pdftitle = {\@title}, pdfauthor = {\@author}}}
\makeatother

% --- Language-related settings ---
%
% these should be modified according to your language

% babelbib for non-english bibliography using bibtex
\usepackage[fixlanguage]{babelbib}
\selectbiblanguage{finnish}

% add bibliography to the table of contents
\usepackage[nottoc]{tocbibind}
% tocbibind renames the bibliography, use the following to change it back
\settocbibname{Lähteet}

% --- Theorem environment definitions ---

\newtheorem{lau}{Lause}
\newtheorem{lem}[lau]{Lemma}
\newtheorem{kor}[lau]{Korollaari}

\theoremstyle{definition}
\newtheorem{maar}[lau]{Määritelmä}
\newtheorem{ong}{Ongelma}
\newtheorem{alg}[lau]{Algoritmi}
\newtheorem{esim}[lau]{Esimerkki}

\theoremstyle{remark}
\newtheorem*{huom}{Huomautus}

\makeatletter
\g@addto@macro\@floatboxreset\centering
\makeatother


% --- tktltiki2 options ---
%
% The following commands define the information used to generate title and
% abstract pages. The following entries should be always specified:

\title{Normalisoitu pakkausetäisyys: sovelluksia ja variaatioita}
\author{Timo Sand}
\date{\today}
\level{Kandidaatintutkielma}
\abstract{
Tähän tulee tutkielman tiivistelmä
\begin{itemize}
  \item NCD:n esittely
  \item Klusteroinnin esittely
  \item Ongelma ja Ominaisuudet
  \item GSD
\end{itemize}
} % TODO: Tiivistelmä

% The following can be used to specify keywords and classification of the paper:

\keywords{Samankaltaisuus}

% classification according to ACM Computing Classification System (http://www.acm.org/about/class/)
% This is probably mostly relevant for computer scientists
% uncomment the following; contents of \classification will be printed under the abstract with a title
% "ACM Computing Classification System (CCS):"
\classification{\textbf{Information systems - Similarity measures} \\ \textit{Theory of computation - Unsupervised learning and clustering} \\ Data mining - Clustering}

% If the automatic page number counting is not working as desired in your case,
% uncomment the following to manually set the number of pages displayed in the abstract page:
%
% \numberofpagesinformation{16 sivua + 10 sivua liitteissä}
%
% If you are not a computer scientist, you will want to uncomment the following by hand and specify
% your department, faculty and subject by hand:
%
% \faculty{Matemaattis-luonnontieteellinen}
% \department{Tietojenkäsittelytieteen laitos}
% \subject{Tietojenkäsittelytiede}
%
% If you are not from the University of Helsinki, then you will most likely want to set these also:
%
% \university{Helsingin Yliopisto}
% \universitylong{HELSINGIN YLIOPISTO --- HELSINGFORS UNIVERSITET --- UNIVERSITY OF HELSINKI} % displayed on the top of the abstract page
% \city{Helsinki}
%

\newcommand{\engl}[1]{\emph{(engl. #1)}}
\newcommand{\kolmogorov}{Kolmogorov-kompleksisuus}


\begin{document}

% --- Front matter ---

\frontmatter      % roman page numbering for front matter

\maketitle        % title page
\makeabstract     % abstract page

\tableofcontents  % table of contents

% --- Main matter ---

\mainmatter       % clear page, start arabic page numbering

\section{Johdanto} % (fold)
\label{sec:johdanto}
\label{par:intro-1}
  Kaikki data on luotu samanveroiseksi, mutta jotkut datat ovat samankaltaisempia kuin toiset.
  Esitämme tavan, jolla ilmaista tämä samankaltaisuus, käyttäen Cilibrasin ja Vitanyin esittelemää samankaltaisuuden metriikkaa \engl{similarity metric}, joka perustuu tiedoston pakkaamiseen. \cite{CV05}
  Metriikka on parametriton, eli se ei käytä datan ominaisuuksia tai taustatietoja, ja sitä voi soveltaa eri aloihin ilman muunnoksia.
  Metriikka on universaali siten, että se approksimoi parametrin, joka ilmaisee parin hallitsevan piirteen samankaltaisuutta pareittain vertailuissa.
  Se on vakaa siinä mielessä, että sen tulokset ovat riippumattomia käytetystä pakkaajasta \cite{CV05}. Pakkaajalla tarkoitetaan pakkausohjelmaa kuten \emph{gzip, ppmz, bzip2}.

  \label{par:intro-2}
  Pakkaukseen perustuva samankaltaisuus \engl{Compression-Based Similarity} on universaali metriikka, jonka kehittivät Cilibrasi ja Vitanyi \cite{CV05}.
  Yksinkertaistettuna tämä tarkoitta, että kaksi objektia ovat lähellä toisiaan, jos voimme ``pakata'' yhden objektin huomattavasti tiiviimmin toisen objektin datalla.
  Abstraktina ideana toimii se, että voimme kuvailla ytimekkäämmin yhden objektin toisen avulla, mikäli objektit ovat samankaltaisia.
  Tämän esittelemme luvussa \ref{sec:normalisoitu_pakkausetaisyys} ja samalla käymme läpi, mihin teoriaan algoritmi perustuu sekä miten se toimii.
  Edellä mainitun vakauden esittämiseen voimme käyttää useaa tosielämän pakkausalgoritmia: tilastollista (PPMZ), Lempel-Ziv -algoritmiin pohjautuvaa hakemistollista (gzip), lohkoperusteista (bzip2) tai erityistarkoitusta varten suunniteltua (Gencompress).

  Kaikissa kohdissa  operoimme $O(log n)$ tarkkuudella, joka on hinta siitä että siirrytään Kolmogorov-kompleksisuudesta laskettavaan approksimaatioon.

  Tarkoituksemme on koota yhteen samankaltaisuuden metriikkaan kaikki tehokkaat etäisyydet \engl{effective distances}: tehokkaat versiot Hammingin etäisyydestä, Euklidisesta etäisyydestä, Lempel-Ziv etäisyydestä ja niin edelleen.
  Tämän metriikan tulee olla niin yleinen, että se toimii yhtäläisesti ja samanaikaisesti kaikilla aloilla: musiikilla, tekstillä, kirjallisuudella, ohjelmilla, genomeilla, luonnollisen kielen määrityksillä.
  Sen pitäisi pystyä samanaikaisesti havaitsemaan kaikki samankaltaisuudet objektien välillä, joita muut etäisyydet havaitsevat erikseen.

  Kun määrittelemme ryhmän sallittavia etäisyyksiä \engl{admissible distances} haluamme sulkea pois epärealistiset, kuten $f(x,y) = \frac{1}{2}$ jokaiselle parille $x \neq y$.
  Saavutamme tämän rajoittamalla objektien lukumäärän annetussa etäisyydessä objektiin.
  Teemme tämän huomioimalla vain todellisia etäisyyksiä seuraavasti: Oletamme kiinnitetyn ohjelmointikielen, joka toimii tutkielman ajan viitekielenä.
  Tämä ohjelmointikieli voi olla yleinen kieli kuten LISP, Ruby tai se voi myös olla kiinnitetty universaali Turingin kone. \cite{CV05,cilibrasi2007google}
  Valinnalla ei kuitenkaan ole merkitystä, sillä teoria on invariantti ohjelmointikielin muutoksille, kunhan pysytään tehdyssä valinnassa.


Jotta voimme soveltaa tätä ideaalia ja tarkkaa matemaattista teoriaa käytäntöön, pitää meidän korvata ei laskettavissa oleva Kolmogorov-kompleksisuus approksimaatiolla käyttäen standardia pakkaajaa.

\label{par:intro-3}
  Luvussa \ref{sec:k_ytt_kohteet} esittelemme algoritmin käyttökohteita monelta eri alueelta.
  Aloitamme siitä, miten yleisesti \emph{Normalisoidun pakkausetäisyyden} (\emph{NCD}) avulla pystymme klusteroimaan tuloksia eri kategorioihin; miten musiikkikappaleet klusteroituvat saman artistin alle, miten kuvantunnistuksessa saamme ryhmitettyä samankaltaiset kuvat ja miten sienten genomeista saamme tarkan lajiryhmityksen.


  Luvun lopussa syvennymme musiikin, kuvantunnistuksen ja dokumenttien kategorisoinnin tuloksiin.


\label{par:intro-4}
  Luvussa \ref{sec:algoritmin_ongelmat_ja_ominaisuudet} esitellään NCD:n ominaisuuksia ja ongelmia.
  Ensiksi esitellään NCD:n kohinansietokykyä, eli katsotaan mitä tapahtuu kun lisätään vähitellen kohinaa toiseen tiedostoista, jota pakataan, ja mittaamalla samankaltaisuutta tämän jälkeen \cite{4167725}.
  Saamme nähdä miten paljon kohina vaikuttaa NCD:n laskemiin etäisyyksiin ja huonontaako se klusteroinnin tuloksia.


  Mikään algoritmi ei ole täydellinen, ja NCD-algoritmillakin on ongelmansa.
  Algoritmissa itsessään ei ole selvää heikkoutta, mutta sen käytössä on otettava pakkaajan valinta huomioon, koska monet suosituista pakkausalgoritmeista ovat optimoituja tietyn kokoisille tiedostoille.
  Niissä on niin kutsuttu ikkunakoko \engl{window size}, joka määrittelee mikä tiedostokoko on sopiva \cite{cebrian2005common}.
  Jos tiedostokoko on pienempi kuin ikkunakoko, niin pakkaus on tehokasta, mutta kun mennään siitä yli, pakkauksesta tulee huomattavasti tehottomampaa.
  Esittelemme tuloksia eri pakkausalgoritmien vertailuista ja mikä näistä algoritmeista on parhaimmaksi havaittu NCD:n kanssa käytettäväksi.


\label{par:intro-5}
  NCD ei ole ainut metriikka, jolla voidaan mitata samankaltaisuutta.
  Internetiä hyödyntäen on tehty metriikka, joka käyttää hakukoneita samankaltaisuuden tutkimiseen; tämä on nimetty Google-samankaltaisuusetäisyydeksi \engl{Google Similarity Distance}.
  Tämä toimii myös muilla hakukoneilla kuten Bing. Luvussa \ref{sec:muita_samankaltaisuuden_metriikoita} esittelemme tämän sekä muita samankaltaisuuden metriikoita.

% section johdanto (end)

\section{Normalisoitu pakkausetäisyys} % (fold)
\label{sec:normalisoitu_pakkausetaisyys}
  \subsection{\kolmogorov} % (fold)
\label{sub:kolmogorov_kompleksisuus}

  Merkkijonon $x$ \emph{\kolmogorov} on pituus bitteinä, lyhimmästä tietokoneohjelmasta joka ilman syötettä palauttaa merkkijonon $x$; tämä merkitään $K(x)=K(x|\lambda)$, $\lambda$ esittää tyhjää syötettä.
  Lyhimmän tietokoneohjelman pituus, joka palauttaa $x$ syötteellä $y$, on \kolmogorov{} $x$:stä syötteellä $y$; tämä merkitään $K(x|y)$.
  Yksi tapa hahmottaa Kolmogorov-kompleksisuutta $K(x)$ on ajatella se pituutena bitteinä $x$:n tiiviimmin pakatussa muodossa, josta voidaan purkaa $x$ pakkauksenpurkuohjelmalla.

% subsection kolmogorov_kompleksisuus (end)
\subsection{Normalisoitu informaatioetäisyys} % (fold)
\label{sub:normalisoitu_informaatioetaisyys}

  Artikkelissa \cite{CV05} on esitelty \emph{informaatioetäisyys} $E(x,y)$, joka on määritelty lyhimpänä binääriohjelman pituutena, joka syötteellä $x$ laskee $y$:n ja syötteellä $y$ laskee $x$:n. Tämä lasketaan seuraavasti:

  \begin{align}
    E(x,y) &= max\{K(x|y),K(y|x)\}.
  \end{align}

  Normalisoitu versio informaatioetäisyydestä $E(x,y)$, jota kutsutaan \emph{normalisoiduksi informaatioetäisyydeksi}, on määritelty seuraavasti

  \begin{align}
    NID(x,y) &= \frac{ max\{K{(x|y)},K{(y|x)}\} }{ max \{K(x),K(y)\}}.
  \end{align}

  Tämä on paras mahdollinen \emph{samankaltaisuuden metriikka}, ja sen on osoitettu \cite{CV05} täyttävän vaatimukset etäisyyden metriikaksi.
  \emph{NID} ei kuitenkaan ole laskettava tai edes semi-laskettava, koska Turingin määritelmän mukaan \kolmogorov{} ei ole laskettava \cite{CV05}.
  Nimittäjän approksimointi annetulla pakkaajalla $C$ on $max \{C(x),C(y)\}$.
  Osoittajan paras approksimaatio on $max\{C(xy),C(yx)\} - \min\{C(x),C(y)\}$ \cite{CV05}.
  Kun \emph{NID} approksimoidaan oikealla pakkaajalla, saadaan tulos jota kutsutaan \emph{normalisoiduksi pakkausetäisyydeksi}.
  Tämä esitellään formaalisti myöhemmin.
% subsection normalisoitu_informaatioetaisyys (end)

\subsection{Normaali pakkaaja} % (fold)
\label{sub:normaali_pakkaaja}

  Seuraavaksi esitämme aksioomia, jotka määrittelevät laajan joukon pakkaajia ja samalla varmistavat \emph{normalisoidussa pakkausetäisyydessä} halutut ominaisuudet.
  Näihin pakkaajiin kuuluvat monet tosielämän pakkaajat.

  Pakkaaja $C$ on \emph{normaali} jos se täyttää seuraavat aksioomat:

  \label{idempotency}
  \begin{enumerate}
    \item \emph{Idempotenssi}: $C(xx) = C(x)$ ja $C(\lambda) = 0$, jossa $\lambda$ on tyhjä merkkijono,
    \item \emph{Monotonisuus}: $C(xy) \geq C(x)$,
    \item \emph{Symmetrisyys}: $C(xy) = C(yx)$ ja
    \item \emph{Distributiivisuus}: $C(xy) + C(z) \leq C(xz) + C(yz)$.
  \end{enumerate}
% subsection normaali_pakkaaja (end)

\subsection{Normalisoitu Pakkausetäisyys} % (fold)
\label{sub:normalisoitu_pakkausetaisyys}

  Jotta voimme soveltaa tätä ideaalia ja tarkkaa matemaattista teoriaa käytäntöön, pitää meidän korvata ei laskettavissa oleva Kolmogorov-kompleksisuus approksimaatiolla käyttäen standardia pakkaajaa.

  Normalisoitua versiota \emph{hyväksyttävästä etäisyydestä} $E_c(x,y)$, joka on pakkaajaan $C$ pohjautuva approksimaatio normalisoidusta informaatioetäisyydestä, kutsutaan nimellä \emph{Normalisoitu Pakkausetäisyys (NCD)} \cite{CV05}.
  Tämä lasketaan seuraavasti

  \begin{align}
    NCD(x,y) &= \frac{C(xy)-min\{C(x),C(y)\}}{max\{C(x),C(y)\}}.
  \end{align}

  \emph{NCD} on funktioiden joukko, joka ottaa argumenteiksi kaksi objektia (esim. tiedostoja tai Googlen hakusanoja) ja tiivistää nämä, erillisinä ja yhdistettyinä.
  Tämä funktioiden joukko on parametrisoitu käytetyn pakkaajan $C$ mukaan.

  Käytännössä \emph{NCD}:n tulos on välillä $0 \leq r \leq 1+ \epsilon$, joka vastaa kahden tiedoston eroa toisistaan; mitä pienempi luku, sitä enemmän tiedostot ovat samankaltaisia.
  Tosielämässä pakkausalgoritmit eivät ole yhtä tehokkaita kuin teoreettiset mallit, joten virhemarginaali $\epsilon$ on lisätty ylärajaan.
  Suurimmalle osalle näistä algoritmeista on epätodennäköistä että  $\epsilon > 0.1$.
% subsection normalisoitu_pakkausetaisyys (end)

  Luonnollinen tulkinta \emph{NCD}:stä, jos oletetaan $C(y) \geq C(x)$, on

  \begin{align}
    NCD(x,y) = \frac{C(xy)-C(x)}{C(y)}.
 \end{align}

  Eli etäisyys $x$:n ja $y$:n välillä on suhde $y$:n parannuksesta, kun $y$ pakataan käyttäen $x$:ää, ja $y$:n pakkauksesta yksinään; suhde ilmaistaan etäisyytenä bittien lukumääränä kummankin pakatun version välillä.

  Kun pakkaaja on normaali, niin \emph{NCD} on normalisoitu hyväksyttävä etäisyys, joka täyttää metriikan yhtälöt, eli se on samankaltaisuuden metriikka.

  Taulukossa \ref{tab:NCD-values} esitellään muutama arvo joilla NCD lasketaan ja mikä tulos näistä saadaan. Taulukon arvot saatu paperista \cite{cebrian2005common}

  \begin{table}[t]
    \begin{tabular}{c|c|l}
      Arvot                    & Laskutoimitus      & NCD:n arvo \\ \hline
      $ C(xx) = 26, C(x) = 17$ & $\frac{26-17}{17} $ & $0.529$    \\ \hline
      $ C(xx) = 33, C(x) = 22$ & $\frac{33-22}{22} $ & $0.5$      \\ \hline
      $ C(xx) = 30, C(x) = 17$ & $\frac{30-17}{17} $ & $0.765$    \\ \hline
      $ C(xx) = 20, C(x) = 14$ & $\frac{20-14}{14} $ & $0.428$    \\ \hline
      $ C(xx) = 16, C(x) = 14$ & $\frac{16-14}{14} $ & $0.143$    \\ \hline
      $ C(xx) = 28, C(x) = 14$ & $\frac{28-14}{14} $ & $1$        \\
    \end{tabular}
    \caption{\emph{Normalisoituja pakkausetäisyyksiä eri syötteille.}}
    \label{tab:NCD-values}
  \end{table}

% section normalisoitu_pakkausetaisyys (end)

\section{Klusterointi} % (fold)
  \label{sec:klusterointi}
    \paragraph{TODO} % (fold)
    \begin{itemize}
      \item Miten klusteroidaan
      \item Etäisyysmatriisi
    \end{itemize}
    \subsection{Etäisyysmatriisi} % (fold)
    \label{sub:etaisyysmatriisi}

    % subsection etaisyysmatriisi (end)
    \subsection{Nelikkomenetelmä} % (fold)
    \label{sub:nelikkomenetelma}

    %"quartet topology" -> "nelikkotopologia",

  %"minimum quartet tree cost" -> "pienin nelikkopuukustannus"

  %"quartet-based tree construction" -> "nelikkopohjainen puunrakennus"
  \paragraph{TODO} % (fold)
    \begin{itemize}
      \item puun rakentaminen etäisyysmatriisista
      \item Puun operaatiot
      \item normalisoitu puun hyötyarvo $S(T)$
      \item Satunnaisuuden käyttö tarkistusiteraatiossa parhaan puun approksimoimiseksi
    \end{itemize}
    % subsection nelikkomenetelma (end)
    \subsection{Tuloksia} % (fold)
    \label{sub:tuloksia}
      \paragraph{TODO} % (fold)
      \begin{itemize}
        \item Musiikin genrepuu
        \item Musiikin pieni vertaus
        \item Musiikin keskikokoinen vertaus
        \item Musiikin suuri vertaus
      \end{itemize}

    % subsection tuloksia (end)
  % section klusterointi (end)

\section{Algoritmin ongelmat ja ominaisuudet} % (fold)
\label{sec:algoritmin_ongelmat_ja_ominaisuudet}
  \subsection{Kohinansietokyky} % (fold)
  \label{sub:kohinansietokyky}

  % TODO: Approximoi differentiaaliyhtälöllä
    Kun NCD:tä käytetään kahteen eri tiedostoon toista näistä voi pitää kohinallisena versiona ensimmäisestä.
    Kohinan lisääminen progressiivisesti tiedostoon voi tuottaa tietoa mittarista \engl{measure} itsestään.
    Tämän vastaavuuden perusteella voimme tehdä teoreettisen päätelmän odotetusta kohinan lisäämisen vaikutuksesta algoritmiin, mikä selittää miksi NCD voi saada suurempia arvoja kuin $1$ joissain tapauksissa. \cite{4167725}

    \begin{figure}[!htb]
      \includegraphics[width=\textwidth]{img/noise-005}
      \caption{\emph{Normalisoitujen pakkausetäisyyksien pohjalta klusteroituja kirjoja eri kirjoittajilta ja vaihteleva määrä kohinaa lisättynä. Merkinnät ovat kirjailijan nimikirjaimet: AC = Agatha Christie, AP = Alexander Pope, EAP = Edgar Allan Poe, WS = William Shakespeare ja NM = Niccolo Machiavelli.}
      % TODO: The quality of the clustering degrades slowly due to the linear growth of the average distortion.
      \cite{4167725}}
      \label{fig:(noise-005)}
    \end{figure}

  % subsection kohinansietokyky (end)

  \subsection{Pakkaajan valinta} % (fold)
  \label{sub:pakkaajan_valinta}

    \begin{figure}[!htb]
      \includegraphics[width=\textwidth]{img/bzip2-best}
      \caption{\emph{Normalisoidut pakkausetäisyydet ensimmäisestä $n$ tavusta, neljälle tiedostolle Calgary Corpus -kokoelmasta, käyttäen \textbf{bzip2} pakkaajaa asetuksella `--best'} \cite{cebrian2005common}}
      \label{fig:(bzip2-best)}
    \end{figure}
    \begin{figure}[!htb]
      \includegraphics[width=\textwidth]{img/bzip2-fast}
      \caption{\emph{Normalisoidut pakkausetäisyydet ensimmäisestä $n$ tavusta, neljälle tiedostolle Calgary Corpus -kokoelmasta, käyttäen \textbf{bzip2} pakkaajaa asetuksella `--fast'} \cite{cebrian2005common}}
      \label{fig:bzip2-fast}
    \end{figure}
    \begin{figure}[!htb]
      \includegraphics[width=\textwidth]{img/gzip-best}
      \caption{\emph{Normalisoidut pakkausetäisyydet ensimmäisestä $n$ tavusta, neljälle tiedostolle Calgary Corpus -kokoelmasta, käyttäen \textbf{gzip} pakkaajaa asetuksella `--best'} \cite{cebrian2005common}}
      \label{fig:gzip-best}
    \end{figure}
    \begin{figure}[!htb]
        \immediate\write18{pdfcrop img/ppmz}
      \includegraphics{img/ppmz}
      \caption{\emph{Normalisoidut pakkausetäisyydet ensimmäisestä $n$ tavusta, neljälle tiedostolle Calgary Corpus -kokoelmasta, käyttäen \textbf{PPMZ} pakkaajaa} \cite{cebrian2005common}}
      \label{fig:ppmz}
    \end{figure}
    \iffalse
     % TODO: This paper shows that the compressors used to compute the normalized compression distance are not idempotent in some cases, being strongly skewed with the size of the objects and window size, and therefore causing a deviation in the identity property of the distance if we don’t take care that the objects to be compressed fit the windows. The relationship underlying the precision of the distance and the size of the objects has been analyzed for several well-known compressors, and specially in depth for three cases, bzip2, gzip and PPMZ which are examples of the three main types of compressors: block-sorting, Lempel-Ziv, and statistic.
    \fi
      NCD vaikuttaa tuottavan vaikuttavia tuloksia klusteroinnissa, mutta tulokset ovat hyvin vahvasti riippuvaisia käytetyn pakkaajan ominaisuuksista.
      Suosittuja pakkaajia \emph{bzip2}, \emph{gzip} ja \emph{PPMZ} on vertailtu NCD:n kanssa \cite{cebrian2005common}, selvittääkseen mitä heikkouksia, jos mitään, näillä on.


      Vertailussa käytettiin Cilibrasin toteuttamaa CompLearn -työkalua \cite{complearn}, josta löytyy bzip2 ja gzip pakkaajat.
      Aineistona käytettiin tunnettua Calgary Corpus -kokoelmaa \cite{calgarycorpus}, joka on 18 tiedoston kokoelma jolla mitataan pakkausalgoritmien suorituskykyä.
      Kokoelmassa on 9 eri tyyppistä tiedostoa, jotta voidaan saada laaja näkemys pakkausalgoritmin toiminnasta.
      Mukana muun muassa on kuva, kirjoja, artikkeleita, lähdekoodia ja tietokoneohjelmia.

      Kaikkia vertailun objekteja käsitellään merkkijonoinen, jotka koostuvat tavuista.
      Jotta voitiin todeta pakkaajan idempotenssin (\ref{idempotency}) pätevyys, kaikki objektit vertailtiin itsensä kanssa.

      Näistä vertailtiin ensiksi bzip2 pakkaajaa, jolle pitää määrittää lohkon koko

    \paragraph{TODO} % (fold)
    \begin{itemize}
      \item bzip2 ja lohkon koko
      \item gzip, liukuva ikkuna ja eteenpäinkatselikkuna
      \item ppmz, hidasta mutta tehokasta, koska ei mitään rajoitteita materiaalin koolle
      \item Lopputulos, jos pyritään klusteroimaan NCD:llä pitäisi aina käyttää PPMZ:taa tai vastaavaa pakkaajaa joka ei rajoita tiedostonkokoa
    \end{itemize}



  % subsection pakkaajan_valinta (end)
% section algoritmin_ongelmat_ja_ominaisuudet (end)

\section{Google-samankaltaisuusetäisyys} % (fold)
  \label{sec:google_similarity_distance}

    % TODO: meaningful
    % TODO: aggregoidun

    Internetin kasvu on houkutellut miljoonia käyttäjiä luomaan miljardeja internetsivuja, jotka ovat sisällöltään ja piirteiltään hyvin vaihtelevia.
    Suunnaton tiedon määrä miltei mistä tahansa aiheesta tekee siitä mahdollisen, että ääripäät kumoutuvat ja täten surin osa tai keskiverto on meaningful heikkolaatuisena approksimaationa.
    Täten on kehitetty yleinen metodi hyödyntämään tätä matalalaatuista tietoa, jota saa ilmaiseksi Internetistä.
    Tämä tietovarasto on kaikille käytettävissä käyttäen mitä tahansa hakukonetta, joka pystyy palauttamaan aggregoidun sivulukumääräarvion, kuten Google.

    Google-samankaltaisuusetäisyys (\emph{GSD}) on hyvin vahvasti verrattavissa Normalisoituun pakkausetäisyyteen \ref{sub:normalisoitu_pakkausetaisyys}, sillä kummatkin algoritmit perustuvat samoihin tekniikoihin, kuten Kolmogorov-kompleksisuuteen (\ref{sub:kolmogorov_kompleksisuus}) ja Normalisoituun informaatioetäisyyteen (\ref{sub:normalisoitu_informaatioetaisyys}).
    Eroavaisuuksia esiintyy siinä, että mitä nämä kaksi algoritmia käyttävät samankaltaisuuden arvioimiseksi.
    Siinä missä NCD pakkaa ja vertaa sisältöä toisiinsa, niin GSD vertaa asioitten nimiä esiintymistiheyteen. \cite{cilibrasi2007google}

    \subsection{Käytännön esimerkki} % (fold)
    \label{sub:kaytannon_esimerkki}
      GSD muodostetaan siten että haetaan ensiksi yhdellä hakutermillä, sitten toisella ja sen jälkeen kummallakin yhdessä.
      Hakujen lukumäärät vertaillaan keskenään ja normalisoidaan ja tästä saadaan samankaltaisuusetäisyys.

      Paperissa \cite{cilibrasi2007google} käytettiin hakusanoja ``horse'' ja ``rider'' esimerkkinä ja vuonna 2007 tuloksena oli $NGD(horse, rider) \approx 0.443$.
      Toistimme kyseisen haun 13.11.2013 ja tuloksena oli $NGD(horse, rider) \approx 0.233$.
      Arvojen suurta eroa on vaikea selvittää ilman laajempaa tutkimusta, mutta vaikuttavia tekoja on internetin kasvu, sekä epäselvyys mikä on tarkka lukumäärä Googlen indeksoituja sivuja.

    % subsection kaytannon_esimerkki (end)

    \subsection{Hakukoneet samankaltaisuuden mittaamisessa} % (fold)
    \label{sub:theory_of_googling_for_similarity}

    % TODO: number of webpages indexed by google

    Googlen indeksoitujen internetsivujen määrä on kasvanut suunnattomaksi ja mikä tahansa yleinen hakutermi esiintyy miljoonilla internetsivulla. Internetin sisällöntuottajiakin on suunnaton määrä ja voidaan olettaa näiden olevan erittäin kuvaava otanta isosta osasta ihmiskuntaa.
    Tämän perusteella voidaan argumentoida että, Google-haun frekvenssi on kuvaava approksimaatio todellisista relatiivisista frekvensseistä yhteiskunnassa.
    Haun tulos on frekvenssi haun tuottamista osumista ja Googlen indeksoitujen sivujen lukumäärästä. \cite{cilibrasi2007google}

    % subsection theory_of_googling_for_similarity (end)
    \subsection{Google-jakauma} % (fold)
    \label{sub:google_jakauma}
      Olkoon yksittäisten Google-hakujen joukko $S$.
      Jatkossa käytämme yksittäisten ja pareittaisten hakujen joukkoja $\{\{x,y\}:x,y \in S\}$.
      Olkoon joukko Googlen indeksoimia internetsivuja $\Omega$.
      Joukon $\Omega$ koko merkitään $M = |\Omega|$.
      Oletamme että jokaisella internetsivulla on yhtä todennäköistä palautua Google-hausta, eli todennäköisyys on $\frac{1}{\Omega}$.
      $\Omega$:n osajoukkoa kutsutaan \emph{tapahtumaksi}.
      Jokainen hakuehto $x$ määrittelee yksittäisen haun tapahtuman $\mathbf{x}\subseteq \Omega$, tämä on joukko internetsivuja joista löytyy $x$ kun suoritetaan Google-haku $x$:llä.
      Olkoon $L: \Omega \rightarrow [0,1]$ tasajakauman todennäköisyysfunktio.
      Tapahtuman $\mathbf{x}$ todennäköisyys on $L(\mathbf{x}) = \frac{|\mathbf{x}|}{M}$.
      Samoin toimii pareittain haun tapahtuma $\mathbf{x}\cap\mathbf{y}\subseteq\Omega$, jossa haetaan pareittain hakuehdoilla $x$ ja $y$.

      Kaikkien tapahtumien todennäköisyydet yhteensä on suurempi kuin $1$, koska tapahtumat menevät päällekkäin, joten tarvitaan normalisointia.
      $|S|$ on yksittäisten ja $\binom{|S|}{2}$ parittaisten hakujen tulosten lukumäärä.
      Määrittelemme
      \begin{align}
        N = \sum_{\{x,y\} \subseteq S} |x \cap y|,
      \end{align}
      yksittäisten ja parittaisten hakujen tulosten lukumäärä yhteensä.
      $N \geq M$, koska jokainen Googlen indeksoima internetsivu sisältää vähintään yhden hakusanan esiintymän. Toisaalta internetsivuilla ei keskivertona ole enempää kuin $\alpha$ hakutermiä, täten $N \leq \alpha{}M$.
      Määrittelemme
      \begin{align}
        g(x) = g(x,x),\, g(x,y) = L(\mathbf{x} \cap \mathbf{y})\frac{M}{N} = \frac{|\mathbf{x}\cap \mathbf{y}|}{N}.
      \end{align}
      Ja sitten on pystytään määrittelemään $\sum_{\{x,y\}\subseteq S} g(x,y) = 1$, eli $g$ määrittelee todennäköisyysjakauman.
    % subsection google_jakauma (end)

    \subsection{Google-samankaltaisuusetäisyys} % (fold)
    \label{sub:google_samankaltaisuusetäisyys}
      \emph{Normalisoitu google-etäisyys} (\emph{NGD}) \engl{Normalized Google Distance}  voidaan määritellä seuraavasti,
      \begin{align}
        NGD(x,y) = \frac{max\{\log{f(x)},\log{f(y)}\}-\log{f(x,y)}}{\log{N}-min\{\log{f(x)},\log{f(y)}\}},
      \end{align}
      % TODO: denotes
      jossa $f(x)$ denotes internetsivujen lukumäärää, jotka sisältävät hakutermin $x$ ja $f(x,y)$ denotes saman mutta sivuille jotka sisältävät termit $x$ ja $y$.
      Tämä normalisoitu google-etäisyys on approksimaation normalisuidusta informaatioetäisyydestä \ref{sub:normalisoitu_informaatioetaisyys}.
      Käytännössä käytämme Googlen palauttamia sivujen lukumääriä frekvensseinä ja valitaan $N$.
      On huomattu että mikä tahansa järkevä arvo voidaan käyttää normalisoinnin arvona $N$.
      NGD:llä on seuraavat ominaisuudet, olettaen että valitaan parametri $N\geq{}M$:
      \begin{enumerate}
        \item NGD:n arvo on väliltä $0$ ja $\infty$ (joskus jopa hieman negatiivisiä, jos Googlen lukumäärät ovat epäluotettavia ja täten $f(x,y) > max\{f(x),f(y)\}$)
        \begin{enumerate}
          \item Jos $x=y$ tai $x\neq{}y$, mutta frekvenssi on
          \[
            f(x) = f(y) = f(x,y) > 0,
          \]
          niin $NGD(x,y)=0$. Eli $x$ ja $y$ ovat Googlen kannalta samat.
          \item Jos frekvenssi $f(x)=0$, sitten jokaiselle hakutermille $y$ pätee $f(x,y)=0$. Tämän lisäksi pätee
          \[
            NGD(x,y)=\frac{\infty}{\infty}
          \]
          jonka ymmärrämme tarkoittavan $1$.
        \end{enumerate}
        \item NGD on aina epänegatiivinen ja $NGD(x,x)=0$ kaikille $x$. Kaikille $x,y$ pätee $NGD(x,y)=NGD(y,x)$, koska se on symmetrinen. NGD ei kuitenkaan ole metriikka, koska se ei täytä ehtoa $NGD(x,y) > 0$ kaikille $x\neq{}y$ \cite{cilibrasi2007google}
        \item NGD on \emph{invariantti skaalautuvuudess} seuraavassa mielessä: Oletamme, että kun Googlen indeksoimien sivujen lukumäärä $N$ kasvaa, niin sivujen, jotka sisältävät annetun hakusanan, lukumäärä lähestyy määrättyä murto-osaa $N$:stä, ja niin myös sivujen lukumäärä, jotka sisältävät hakusanojen parin.
      \end{enumerate}
    % subsection google_samankaltaisuuset_isyys (end)

    \subsection{Hierarkinen klusterointi} % (fold)
    \label{sub:klusterointi}
      Klusterointi normalisoidun google-etäisyyden avulla onnistuu samanlailla kuin normalisoidulla pakkausetäisyydelläkin. Ensiksi lasketaan etäisyysmatriisi \ref{sub:etaisyysmatriisi} hakuehtojen pareittaisista NGD:n arvoista ja sitten nelikkomenetelmää \ref{sub:nelikkomenetelma} käyttäen rakennetaan \emph{juureton ternääripuu} \engl{unrooted ternary tree}.

      Esimerkkinä NGD:n klusteroinnista käytettiin 1600-luvun Hollantilaisten maalareitten teoksia. Kuvassa \ref{fig:dutch-paintings} on ternääripuu valituista 15 maalauksesta Rembrandtilta, Steeniltä ja Bolilta. Hakusanoina käytettiin ainoastaan maalausten koko nimiä. Seuraavaksi lista maalauksista ja teoksista

      \begin{itemize}
        \item \textbf{Rembrandt van Rijn:} \emph{Hendrickje slapend, Portrait of Maria Trip, Portrait of Johannes Wtenbogaert, The Stone Bridge, and The Prophetess Anna.}
        \item \textbf{Jan Steen:} \emph{Leiden Baker Arend Oostwaert, Keyzerswaert, Two Men Playing Backgammon, Woman at her Toilet, Prince’s Day, and The Merry Family.}
        \item \textbf{Ferdinand Bol:} \emph{Maria Rey, Consul Titus Manlius Torquatus, Swartenhout, and Venus and Adonis.}
      \end{itemize}

      \begin{figure}[!htb]
        \immediate\write18{pdfcrop img/dutch-paintings}
        \includegraphics[width=\textwidth, height=350pt]{img/dutch-paintings}
        \caption{\emph{Hierarkinen klusterointi Hollantilaisista maalauksista} \cite{cilibrasi2007google} }
        \label{fig:dutch-paintings}
      \end{figure}



    % subsection klusterointi (end)


  % section google_similarity_distance (end)

\section{``Loppukaneetti''} % (fold)
\label{sec:loppukaneetti}

\begin{itemize}
  \item Nähty että toimii klusterointiin
  \item riippumaton tiedosta
\end{itemize}

Kuten esitimme aiemmin niin NCD toimii edelleen hyvin samankaltaisuuden mittarina vaikka syötteessä olisi kohinaa.

Vaikka NCD:n kehittäjät luulivat osoittaneen, että algoritmi on riippumaton pakkaajan valinnasta \cite{CV05}, niin vertailujen tuloksena on pystytty todistamaan, että ainaostaan tiedostonkokoa rajoittamaton pakkaaja toimii aidosti NCD:n kanssa \ref{sub:pakkaajan_valinta}.
Kannattavin valinta pakkaajaksi on \emph{PPMZ}, mikäli käyttää NCD:n tuloksia klusterointiin.

\paragraph{Muita klusteroinnin menetelmiä} % (fold)
\label{par:muita_klusteroinnin_menetelmia}
  NCD ei tietenkään ole ainut menetelmä jota Unsupervised learningissä voi käyttää datan klusteroimiseen.
  Vaihtoehtoisia menetelmiä ovat muunmuassa: Hierarkinen klusterointi, joka ..., K-keskiä klusterointi, joka ... ja ...
  \begin{itemize}
    \item Hierarkinen klusterointi % TODO: http://en.wikipedia.org/wiki/Hierarchical_clustering
    \item K-keskiö klusterointi % TODO: http://en.wikipedia.org/wiki/K-means_clustering
  \end{itemize}
% paragraph muita_klusteroinnin_menetelmia (end)

\paragraph{Aiheesta lisää luettavaa} % (fold)
\label{par:aiheesta_lisaa_luettavaa}
  Normalisoidun pakkausetäisyyden aihealueesta on tehty paljon tutkimusta eri suuntautumishaaroihin, lisätietoa NCD:n käytöstä genetiikassa löytyy paperista Geeniekspressioanalyysi \cite{nykter2005normalized}.
  Tarkemppa tietoa normalisoidun informaatioetäisyyden ominaisuuksista ja sen epäapproksimoitavuudesta \cite{terwijn2011nonapproximability}.
  NCD:tä käytetään myös erilaisissa kuvantunnistus ongelmissa, asiasta lisää \cite{doi:10.1117/12.704334}.
  NCD:n kohinan sietokykyä on tutkittu ja se on avannut tutkimuksen mahdollisuuksia tälle aiheelle, tämän lisäksi on myös tutkittu miten NCD:tä voi hyödyntää kohinan poistossa \cite{vitanyi2013similarity}

% paragraph aiheesta_lisaa_luettavaa (end)

% section loppukaneetti (end)
\pagebreak
% --- References ---
%
% bibtex is used to generate the bibliography. The babplain style
% will generate numeric references (e.g. [1]) appropriate for theoretical
% computer science. If you need alphanumeric references (e.g [Tur90]), use
%
\bibliographystyle{babalpha-lf}
%
% instead.

% \bibliographystyle{babplain-lf}
\bibliography{references-fi}


% --- Appendices ---

% uncomment the following

% \newpage
% \appendix
%
% \section{Esimerkkiliite}

\end{document}
