% --- Template for thesis / report with tktltiki2 class ---
%
% last updated 2013/02/15 for tkltiki2 v1.02

\documentclass[11pt,finnish]{tktltiki2}

% tktltiki2 automatically loads babel, so you can simply
% give the language parameter (e.g. finnish, swedish, english, british) as
% a parameter for the class: \documentclass[finnish]{tktltiki2}.
% The information on title and abstract is generated automatically depending on
% the language, see below if you need to change any of these manually.
%
% Class options:
% - grading                 -- Print labels for grading information on the front page.
% - disablelastpagecounter  -- Disables the automatic generation of page number information
%                              in the abstract. See also \numberofpagesinformation{} command below.
%
% The class also respects the following options of article class:
%   10pt, 11pt, 12pt, final, draft, oneside, twoside,
%   openright, openany, onecolumn, twocolumn, leqno, fleqn
%
% The default font size is 11pt. The paper size used is A4, other sizes are not supported.
%
% rubber: module pdftex

% --- General packages ---

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsfonts,amsmath,amssymb,amsthm,booktabs,color,enumitem,graphicx}
\usepackage[pdftex,hidelinks]{hyperref}

\usepackage{setspace}
\onehalfspacing

% Automatically set the PDF metadata fields
\makeatletter
\AtBeginDocument{\hypersetup{pdftitle = {\@title}, pdfauthor = {\@author}}}
\makeatother

% --- Language-related settings ---
%
% these should be modified according to your language

% babelbib for non-english bibliography using bibtex
\usepackage[fixlanguage]{babelbib}
\selectbiblanguage{finnish}

% add bibliography to the table of contents
\usepackage[nottoc]{tocbibind}
% tocbibind renames the bibliography, use the following to change it back
\settocbibname{Lähteet}

% --- Theorem environment definitions ---

\newtheorem{lau}{Lause}
\newtheorem{lem}[lau]{Lemma}
\newtheorem{kor}[lau]{Korollaari}

\theoremstyle{definition}
\newtheorem{maar}[lau]{Määritelmä}
\newtheorem{ong}{Ongelma}
\newtheorem{alg}[lau]{Algoritmi}
\newtheorem{esim}[lau]{Esimerkki}

\theoremstyle{remark}
\newtheorem*{huom}{Huomautus}


% --- tktltiki2 options ---
%
% The following commands define the information used to generate title and
% abstract pages. The following entries should be always specified:

\title{Normalisoitu pakkausetäisyys}
\author{Timo Sand}
\date{\today}
\level{Kandidaatintutkielma}
\abstract{Aine}

% The following can be used to specify keywords and classification of the paper:

\keywords{avainsana 1, avainsana 2, avainsana 3}

% classification according to ACM Computing Classification System (http://www.acm.org/about/class/)
% This is probably mostly relevant for computer scientists
% uncomment the following; contents of \classification will be printed under the abstract with a title
% "ACM Computing Classification System (CCS):"
% \classification{}

% If the automatic page number counting is not working as desired in your case,
% uncomment the following to manually set the number of pages displayed in the abstract page:
%
% \numberofpagesinformation{16 sivua + 10 sivua liitteissä}
%
% If you are not a computer scientist, you will want to uncomment the following by hand and specify
% your department, faculty and subject by hand:
%
% \faculty{Matemaattis-luonnontieteellinen}
% \department{Tietojenkäsittelytieteen laitos}
% \subject{Tietojenkäsittelytiede}
%
% If you are not from the University of Helsinki, then you will most likely want to set these also:
%
% \university{Helsingin Yliopisto}
% \universitylong{HELSINGIN YLIOPISTO --- HELSINGFORS UNIVERSITET --- UNIVERSITY OF HELSINKI} % displayed on the top of the abstract page
% \city{Helsinki}
%


\begin{document}

% --- Front matter ---

\frontmatter      % roman page numbering for front matter

\maketitle        % title page
% \makeabstract     % abstract page

\tableofcontents  % table of contents

% --- Main matter ---

\mainmatter       % clear page, start arabic page numbering

\section{Johdanto} % (fold)
\label{sec:johdanto}
\iffalse
  All data are created equal but some data are more alike than others. We propose a method expressing this alikeness, using a new similarity metric based on compression. It is parameter-free in that it does not use any features or background knowledge about the data, and can without changes be applied to different areas and across area boundaries. It is universal in that it approximates the parameter expressing similarity of the dominant feature in all pairwise comparisons. It is robust in the sense that its success appears independent from the type of compressor used. The clustering we use is hierarchical clustering indendrograms based on a new fast heuristic for the quartet method. The method is available as an open-source software tool. Below we explain the method, the theory underpinning it, and present evidence for its universality and robustness by experiments and results in a plethora of different areas using different types of compressors.

  Non-Feature Similarities: Our aim is to capture, in a single simi- larity metric, every effective distance: effective versions of Hamming distance, Euclidean distance, edit distances, alignment distance, Lempel–Ziv distance [11], and so on. This metric should be so general that it works in every domain: music, text, literature, programs, genomes, executables, natural language determination, equally and si- multaneously. It would be able to simultaneously detect all similarities betweenpiecesthatothereffectivedistancescandetectseperately.

  Compression-Based Similarity: Such a “universal” metric was co-developed by us in, as a normalized version of the “information metric” of [4], [32]. Roughly speaking, two objects are deemed close if we can significantly “compress” one given the information in theother,theideabeingthatiftwopiecesaremoresimilar,thenwe can more succinctly describe one given the other. The mathematics used is based on Kolmogorov complexity theory [32]. In [31], we defined a new class of (possibly nonmetric) distances, taking values in $[0,1]$ and appropriate for measuring effective similarity relations between sequences, say one type of similarity per distance, and vice versa.Itwasshownthatanappropriately“normalized”information distance minorizes every distance in the class. It discovers all effective similaritiesinthesensethatiftwoobjectsarecloseaccordingtosome effective similarity, then they are also close according to the normal- ized information distance. Put differently, the normalized information distance represents similarity according to the dominating shared featurebetweenthetwoobjectsbeingcompared.Incomparisonsof more than two objects, different pairs may have different dominating features. The normalized information distance is a metric and takes valuesin$[0,1]$;hence,itmaybecalledthesimilaritymetric.
  To applythisidealprecisemathematicaltheoryinreallife,wehaveto replace the use of the noncomputable Kolmogorov complexity by an approximation using a standard real-world compressor. Earlier approaches resulted in the first completely automatic construction of the phylogeny tree based on whole mitochondrial genomes, [29]–[31], a completely automatic construction of a language tree for over 50 Euro-Asian languages [31], detects plagiarism in student programming assignments[8],givesphylogenyofchainletters[5],andclusters music [10]. Moreover, the method turns out to be robust under change of the underlying compressor-types: statistical (PPMZ), Lempel–Ziv based dictionary (gzip), block based (bzip2), or special purpose (Gencompress).
\fi
\paragraph{} % (fold)
\label{par:intro-1}
  Kaikki data on luotu samanveroiseksi, mutta jotkut ovat samankaltaisempia kuin toiset. Esitämme tavan jolla esittää tämä samankaltaisuus, käyttäen uutta samankaltaisuuden metriikkaa(engl. similarity metric) joka perustuu tiedoston pakkaamiseen. Se on parametriton siten että se ei käytä ominaisuuksia tai taustatieto datasta, ja sitä voi soveltaa eri alueisiin ja aluerajojen yli ilman muunnoksia. Se on yleinen siten että se approksimoi parametrin joka esittää samankaltaisuutta hallitsevassa piirteessä kaikissa pareittain vertailuissa. Se on tukeva siinä mielessä, että sen menestys esiintyy riippumatta siitä mitä kompressoria käytetään.

% paragraph  (end)

\paragraph{} % (fold)
\label{par:intro-2}
  Seuraavaksi esittelemme algoritmin toimintaperiaatteen ja sen koostumuksen.

% paragraph  (end)

\paragraph{} % (fold)
\label{par:intro-3}
  Luvussa 3 esittelemme algoritmin käyttökohteita monelta eri alueelta

% paragraph  (end)

\paragraph{} % (fold)
\label{par:intro-4}
  Luvussa 4 esittelmme algoritmin ominaisuuksia ja ongelmia (Kohinansieto, kompressorin valinta)

% paragraph  (end)

\paragraph{} % (fold)
\label{par:intro-5}
  Luvussa 5 kosketamme muita samankaltaisuuden metriikoita kuten Google Similarity Distance

% paragraph  (end)

% section johdanto (end)

\section{Normalisoitu Pakkausetäisyys} % (fold)
\label{sec:normalisoitu_pakkauset_isyys}
  \paragraph{Kolmogorov kompleksisuus} % (fold)
\label{par:kolmogorov_kompleksisuus}

  % Esitellään K(x) ensin
  Lyhimmän binääriohjelman pituus, joka palauttaa $x$ syötteellä $y$, on \emph{Kolmogorov kompleksisuus} $x$:stä syötteellä $y$; tämä merkitään $K(x|y)$. Pohjimmillaan Kolmogorov kompleksisuus tiedostosta on sen äärimmäisesti pakatun version pituus.

% paragraph kolmogorov_kompleksisuus (end)
\paragraph{Normalisoitu Informaatioetäisyys} % (fold)
\label{par:normalisoitu_informaatioet_isyys}

  Artikkelissa \cite{CV05} on esitelty \emph{informaatioetäisyys} $E(x,y)$, joka on määritelty lyhimpänä binääriohjelmana, joka syötteellä $x$ laskee $y$:n ja syötteellä $y$ laskee $x$:n. Tämä lasketaan seuraavasti \cite{10.1109/WDM.2004.1358107}

  \begin{align}
    E(x,y) &= max\{K(x|y),K(y|x)\}.
  \end{align}

  Normalisoitu versio informaatioetäisyydestä ($E(x,y)$), jota kutsutaan \emph{normalisoiduksi informaatioetäisyydeksi}, on määritelty seuraavasti

  \begin{align}
    NID(x,y) &= \frac{ max\{K{(x|y)},K{(y|x)}\} }{ max \{K(x),K(y)\}}.
  \end{align}

  % NID on paras mahdollinen etäisyyden metriikka
  Tätä kutsutaan \emph{samankaltaisuuden metriikaksi}, koska tämän on osoitettu \cite{CV05} täyttävän vaatimukset etäisyyden metriikaksi. \emph{NID} ei kuitenkaan ole laskettavissa tai edes semi-laskettavissa, koska Turingin määritelmän mukaan Kolmogorov kompleksisuus ei ole laskettavissa.
  Nimittäjän approksimointi annetulla kompressorilla $C$ on $max \{C(x),C(y)\}$. Osoittajan paras approksimaatio on $max\{C(xy),C(yx)\} - \min\{C(x),C(y)\}$ \cite{CV05}.
  Kun \emph{NID} approksimoidaan oikealla kompressorilla, saadaa tulos jota kutsutaan \emph{normalisoiduksi pakkausetäisyydeksi}. Tämä esitellään formaalisti myöhemmin.
% paragraph normalisoitu_informaatioet_isyys (end)

\paragraph{Normaali Kompressori} % (fold)
\label{par:normaali_kompressori}

  Seuraavaksi esitämme aksioomia, jotka määrittelevät laajan joukon kompressoreita ja samalla varmistavat \emph{normalisoidussa pakkausetäisyydessä} halutut ominaisuudet. Näihin kompressoreihin kuuluvat monet tosielämän kompressorit.

  Kompressori $C$ on \emph{normaali} jos se täyttää seuraavat aksioomat, $O(log n)$ termiin saakka:

  \begin{enumerate}
    \item \emph{Idempotenssi}: $C(xx) == C(x)$ ja $C(\lambda) = 0$, jossa $\lambda$ on tyhjä merkkijono,
    \item \emph{Monotonisuus}: $C(xy) \geq C(x)$,
    \item \emph{Symmetrisuus}: $C(xy) == C(yx)$ ja
    \item \emph{Distributiivisuus}: $C(xy) + C(z) \leq C(xz) + C(yz)$.
  \end{enumerate}
% paragraph normaali_kompressori (end)

\paragraph{Normalisoitu Pakkausetäisyys} % (fold)
\label{par:normalisoitu_pakkauset_isyys}

  Normalisoitua versiota \emph{hyväksyttävästä etäisyydestä} $E_c(x,y)$, joka on kompressoriin $C$ pohjautuva approksimaatio \emph{normalisoidusta informaatioetäisyydestä}, kutsutaan nimellä \emph{Normalisoitu Pakkausetäisyys (NCD)} \cite{CV05}. Tämä lasketaan seuraavasti

  \begin{align}
    NCD(x,y) &= \frac{C(xy)-min\{C(x),C(y)\}}{max\{C(x),C(y)\}}.
  \end{align}

  \emph{NCD} on funktioden joukko, joka ottaa argumenteiksi kaksi objektia (esim. tiedostoja tai Googlen hakusanoja) ja tiivistää nämä, erillisinä ja yhdistettyinä. Tämä funktioden joukko on parametrisoitu käytetyn kompressorin $C$ mukaan.

  Käytännössä \emph{NCD}:n tulos on välillä $0 \leq r \leq 1+ \epsilon$, joka vastaa kahden tiedoston eroa toisistaan; mitä pienempi luku, sitä enemmän tiedostot ovat samankaltaisia. Tosielämässä pakkausalgoritmit eivät ole yhtä tehokkaita kuin teoreettiset mallit, joten virhemarginaali $\epsilon$ on lisätty ylärajaan. Suurimmalle osalle näistä algoritmeistä on epätodennäköistä että  $\epsilon > 0.1$.
% paragraph normalisoitu_pakkauset_isyys (end)

  Luonnollinen tulkinta \emph{NCD}:stä, jos oletetaan $C(y) \geq C(x)$, on

  \begin{align}
    NCD(x,y) = \frac{C(xy)-C(x)}{C(y)}.
 \end{align}

  Eli etäisyys $x$:n ja $y$:n välillä on suhde $y$:n parannuksesta kun $y$ pakataan käyttäen $x$:ää, ja $y$:n pakkauksesta yksinään; suhde ilmaistaan etäisyytenä bittien lukumääränä kummankin pakatun version välillä.

  Kun kompressori on normaali niin \emph{NCD} on normalisoitu hyväksyttävä etäisyys, joka täyttää metriikan yhtälöt, eli se on samankaltaisuuden metriikka.


  % O(log n) alkuun "hinta siitä että siirrytänä Kolmogorov-kompleksisuudesta laskettavaan"
% section normalisoitu_pakkauset_isyys (end)

\section{Klusterointi} % (fold)
\label{sec:klusterointi}

% section klusterointi (end)

\section{Käyttökohteet} % (fold)
\label{sec:k_ytt_kohteet}
  \subsection{Klusteroinnin tulokset} % (fold)
  \label{sub:klusteroinnin_tulokset}

  % subsection klusteroinnin_tulokset (end)
  \subsection{Kuvantunnnistus} % (fold)
  \label{sub:kuvantunnnistus}

  % subsection kuvantunnnistus (end)
% section k_ytt_kohteet (end)

\section{Kohinansietokyky} % (fold)
\label{sec:kohinan_sietokyky}

Kun NCD:tä käytetään kahteen eri tiedostoon toista näistä voi pitää kohinallisena versiona ensimmäisestä. Progressivisen kohinan lisääminen tiedostoon voi tuottaa tietoa mittarista(measure) itsestään. Tämän vastaavuuden perusteella voimme tehdä teoreettisen päätelmän odotetusta kohinan lisäämisen vaikutuksesta algoritmiin, mikä selittää miksi NCD voi saada suurempia arvoja kuin $1$ joissain tapauksissa. \cite{4167725}

% section kohinan_sietokyky (end)

\section{Huomioitavaa kompressorin valitsemisessa} % (fold)
\label{sec:huomioitavaa_kompressorin_valitsemisessa}

% section huomioitavaa_kompressorin_valitsemisessa (end)

\section{Muita samankaltaisuuden metriikoita} % (fold)
\label{sec:muita_samankaltaisuuden_metriikoita}
  \subsection{Google Similarity Distance} % (fold)
  \label{sub:google_similarity_distance}

  % subsection google_similarity_distance (end)
% section muita_samankaltaisuuden_metriikoita (end)
% --- References ---
%
% bibtex is used to generate the bibliography. The babplain style
% will generate numeric references (e.g. [1]) appropriate for theoretical
% computer science. If you need alphanumeric references (e.g [Tur90]), use
%
\bibliographystyle{babalpha-lf}
%
% instead.

% \bibliographystyle{babplain-lf}
\bibliography{references-fi}


% --- Appendices ---

% uncomment the following

% \newpage
% \appendix
%
% \section{Esimerkkiliite}

\end{document}
